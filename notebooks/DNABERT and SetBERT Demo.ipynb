{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SetBERT Demo\n",
    "\n",
    "This notebook serves to demonstrate how to pull a pretrained SetBERT model from W&B and fine tune it to a specific down-stream task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install dnadb tqdm tf-settransformer tf-utilities wandb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable Tensorflow info logging\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '2'\n",
    "\n",
    "# Add the deepdna source code to the path\n",
    "import sys\n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dnadb import dna, fasta\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import tf_utilities as tfu\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import wandb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')],\n",
       " [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfu.devices.select_gpu(0)\n",
    "# tfu.devices.select_cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User's home directory\n",
    "HOME_PATH = Path('~').expanduser()\n",
    "\n",
    "# Path to the dataset\n",
    "DATASET_PATH = HOME_PATH / \"work/Datasets/Walker_Reed_OTU\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepdna.data.dataset import Dataset\n",
    "from deepdna.data.otu import OtuSampleDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(DATASET_PATH)\n",
    "\n",
    "fasta_dbs = list(map(fasta.FastaDb, dataset.fasta_dbs(Dataset.Split.Train)))\n",
    "otu_dbs = list(map(OtuSampleDb, dataset.otu_dbs(Dataset.Split.Train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FastaEntry(identifier='M03064_59_000000000-JCVRG_1_2104_6980_18698', sequence='TACGTATGGGGCGAGCGTTGTTCGGAGTTATTGGGCGTAAAGCGCGTGTAGGCGGTTTTTTAAGTCTGATGTGAAAGCCCCGGGCTCAACCTGGGAAGTGCATTGGATACTGGAAGACTTGAGTACGGGAGAGGGTAGTGGAATTCCTAGTGTAGGAGTGAAATCCGTAGATATTAGGAGGAACACCGGTGGCGAAGGCGGCTGCCTGGACCGATACTGACGCTGAGACGCGAAAGCGTGGGGAGCAAACAGG', extra='wet710t\\tOtu0000093\\tNumRep=4')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasta_dbs[0][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Pretrained SetBERT Model\n",
    "\n",
    "Pretrained models are stored in as Weights & Biases artifacts. Here we pull it using the W&B API.\n",
    "\n",
    "The available pretrained SetBERT artifacts are available [here](https://wandb.ai/sirdavidludwig/setbert-pretrain/artifacts/model/setbert-pretrain-reed-128d-250l)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the Artifact\n",
    "\n",
    "Here we use a pretrained SetBERT model trained using sequences of length 250. The artifact page is available [here](https://wandb.ai/sirdavidludwig/setbert-pretrain/artifacts/model/setbert-pretrain-reed-128d-250l/v0/overview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SETBERT_ARTIFACT = \"sirdavidludwig/setbert-pretrain/setbert-pretrain-reed-abund-128d-250l:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact setbert-pretrain-reed-abund-128d-250l:latest, 91.00MB. 4 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   4 of 4 files downloaded.  \n",
      "Done. 0:0:0.1\n"
     ]
    }
   ],
   "source": [
    "pretrained_setbert_path = api.artifact(SETBERT_ARTIFACT).download()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our own DNABERT and SetBERT modules\n",
    "from deepdna.nn.models import load_model, setbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_setbert_model = load_model(\n",
    "    pretrained_setbert_path,\n",
    "    setbert.SetBertPretrainModel # optionally specify the model type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<deepdna.nn.models.setbert.SetBertPretrainModel at 0x7f35bf3640a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_setbert_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DNABERT is embedded in the SetBERT model and can be extracted like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<deepdna.nn.models.dnabert.DnaBertEncoderModel at 0x7f35bf3642e0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnabert_encoder = pretrained_setbert_model.base.dnabert_encoder\n",
    "dnabert_encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Constructing a Downstream SetBERT Task\n",
    "\n",
    "For a simple downstream task, we will look at predicting the sample that the subsample comes from."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator\n",
    "\n",
    "First we need to create the data generator that can provide the model with the appropriate information. The available data generators are located in `src/deepdna/nn/data_generators.py` for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepdna.nn.data_generators import OtuSequenceGenerator\n",
    "\n",
    "class OtuSampleGenerator(OtuSequenceGenerator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sample: tuple[fasta.FastaDb, OtuSampleDb],\n",
    "        sequence_length: int,\n",
    "        kmer: int,\n",
    "        use_presence_absence: bool = False,\n",
    "        batch_size: int = 16,\n",
    "        batches_per_epoch: int = 100,\n",
    "        subsample_size: int = 0,\n",
    "        augment_slide: bool = True,\n",
    "        augment_ambiguous_bases: bool = True,\n",
    "        encoder_batch_size: int = 1,\n",
    "        rng: np.random.Generator = np.random.default_rng()\n",
    "    ):\n",
    "        super().__init__(\n",
    "            samples=[sample],\n",
    "            sequence_length=sequence_length,\n",
    "            kmer=kmer,\n",
    "            use_presence_absence=use_presence_absence,\n",
    "            batch_size=batch_size,\n",
    "            batches_per_epoch=batches_per_epoch,\n",
    "            subsample_size=subsample_size,\n",
    "            augment_slide=augment_slide,\n",
    "            augment_ambiguous_bases=augment_ambiguous_bases,\n",
    "            rng=rng\n",
    "        )\n",
    "        self.encoder_batch_size = encoder_batch_size\n",
    "        self.encoder = dnabert_encoder\n",
    "\n",
    "    def generate_batch(self, rng: np.random.Generator):\n",
    "        (_, sample_indices), entries = self.sampler.random_entries(\n",
    "            self.batch_size, max(1, self.subsample_size), rng)\n",
    "        sequences = self.sampler.sequences(entries, rng)\n",
    "        if self.kmer > 1:\n",
    "            sequences = dna.encode_kmers(\n",
    "                sequences, # type: ignore\n",
    "                self.kmer,\n",
    "                not self.sampler.augment_ambiguous_bases) # type: ignore\n",
    "        if self.subsample_size == 0:\n",
    "            sequences = np.squeeze(sequences, axis=1)\n",
    "        return sequences, sample_indices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Data Generator Instances for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = OtuSampleGenerator(\n",
    "    (fasta_dbs[0], otu_dbs[0]),\n",
    "    sequence_length=dnabert_encoder.base.sequence_length,\n",
    "    kmer=dnabert_encoder.base.kmer,\n",
    "    use_presence_absence=False,\n",
    "    subsample_size=1000,\n",
    "    batch_size=8,\n",
    "    batches_per_epoch=100\n",
    ")\n",
    "\n",
    "# test_data = OtuSampleGenerator(\n",
    "#     (fasta_dbs[0], otu_dbs[0]),\n",
    "#     sequence_length=dnabert_encoder.base.sequence_length,\n",
    "#     kmer=dnabert_encoder.base.kmer,\n",
    "#     use_presence_absence=False,\n",
    "#     subsample_size=1000,\n",
    "#     shuffle=False,\n",
    "#     batch_size=8,\n",
    "#     batches_per_epoch=100\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples_to_predict = len(otu_dbs[0])\n",
    "num_samples_to_predict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to use the set embeddings that come from the SetBERT model, we'll employ a SetBERT encoder model. This model simply extracts and returns the class token representing the embedded set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "setbert_encoder = setbert.SetBertEncoderModel(pretrained_setbert_model.base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 1000, 128)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setbert_encoder.input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8, 128), dtype=float32, numpy=\n",
       "array([[ 0.5711318 ,  4.1046515 , -1.8908697 , ..., -6.3806534 ,\n",
       "         8.200887  ,  0.8826097 ],\n",
       "       [-0.82003796,  4.9176493 , -1.9321549 , ..., -6.8010664 ,\n",
       "         7.000064  ,  1.519652  ],\n",
       "       [-0.14095283,  4.4001226 , -2.5025759 , ..., -6.810854  ,\n",
       "         7.4469137 ,  1.351651  ],\n",
       "       ...,\n",
       "       [ 0.1294086 ,  4.2874637 , -2.6301215 , ..., -6.940845  ,\n",
       "         7.9330893 ,  2.1062567 ],\n",
       "       [-0.6797569 ,  4.781932  , -2.0586112 , ..., -6.6996155 ,\n",
       "         7.3001323 ,  1.1681182 ],\n",
       "       [ 0.4470997 ,  3.7609084 , -2.4623172 , ..., -7.1823206 ,\n",
       "         8.035939  ,  1.4849747 ]], dtype=float32)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Outputs 8 128d embeddings, one for each sample\n",
    "setbert_encoder(train_data[0][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create the full model by encoding the input and passing the set embeddings through a single dense layer with a softmax activation function to compute predict which sample the input originated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 248)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsample_length = setbert_encoder.base.max_set_len\n",
    "kmer_sequence_length = dnabert_encoder.input_shape[1]\n",
    "\n",
    "subsample_length, kmer_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input((subsample_length, kmer_sequence_length))\n",
    "class_tokens = setbert_encoder(inputs)\n",
    "sample_predictions = tf.keras.layers.Dense(num_samples_to_predict, activation=\"softmax\")(class_tokens)\n",
    "model = tf.keras.Model(inputs, sample_predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile it using the appropriate loss function/metrics/optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions tf.Tensor([619 619 619 619 619 619 619 619], shape=(8,), dtype=int64)\n",
      "Targets [272 276 596 356 232 352 577 186]\n"
     ]
    }
   ],
   "source": [
    "# Grab a batch from the training data\n",
    "batch = train_data[0]\n",
    "\n",
    "# Get the inputs and targets from the batch\n",
    "inputs = batch[0]\n",
    "targets = batch[1]\n",
    "\n",
    "predictions = model(inputs)\n",
    "\n",
    "# Test predictions of untrained model\n",
    "print(\"Predictions\", tf.argmax(predictions, axis=1))\n",
    "print(\"Targets\", targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_data, \n",
    "    # validation_data=test_data, \n",
    "    epochs=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
