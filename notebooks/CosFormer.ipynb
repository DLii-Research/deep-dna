{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "690a0e41-01c1-450f-945a-8f2ee6affe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tf_utils as tfu\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8178f980-ed33-49a6-91aa-74f4748928e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-01 18:42:46.036965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-01 18:42:46.037337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-01 18:42:46.041597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-01 18:42:46.041954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-01 18:42:46.042706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-01 18:42:46.043034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-01 18:42:46.044612: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "strategy = tfu.strategy.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e69335ff-44f2-41d9-983b-17146175274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@CustomObject\n",
    "class CosFormerMultiHeadAttention(keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 embed_dim,\n",
    "                 num_heads,\n",
    "                 causal=False,\n",
    "                 act_fun=\"relu\",\n",
    "                 **kwargs):\n",
    "        super(CosFormerMultiHeadAttention, self).__init__(**kwargs)\n",
    "        \n",
    "        assert embed_dim % num_heads == 0\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.causal = causal\n",
    "        self.act_fun = act_fun\n",
    "        \n",
    "        self.q_proj = keras.layers.Dense(embed_dim, activation=act_fun)\n",
    "        self.k_proj = keras.layers.Dense(embed_dim, activation=act_fun)\n",
    "        self.v_proj = keras.layers.Dense(embed_dim)\n",
    "        self.out_proj = keras.layers.Dense(embed_dim)\n",
    "        \n",
    "    def get_index(self, seq_len):\n",
    "        return np.pi / 2 * tf.reshape(tf.range(1, seq_len + 1, dtype=tf.float32), (1, -1, 1))\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.head_dim))\n",
    "        return tf.reshape(tf.transpose(x, (0, 2, 1, 3)), (batch_size*self.num_heads, -1, self.head_dim))\n",
    "    \n",
    "    def call(self, query, value=None, key=None, eps=1e-6):\n",
    "        if key is None:\n",
    "            key = query\n",
    "        if value is None:\n",
    "            value = query\n",
    "        \n",
    "        batch_size = tf.shape(query)[0]\n",
    "        tgt_len = tf.shape(query)[1]\n",
    "        src_len = tf.shape(key)[1]\n",
    "\n",
    "        q = self.q_proj(query)\n",
    "        k = self.k_proj(key)\n",
    "        v = self.v_proj(value)\n",
    "\n",
    "        # multi-head reshape\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        # cos transform\n",
    "        m = tf.cast(tf.maximum(src_len, tgt_len), dtype=tf.float32)\n",
    "        weight_index = self.get_index(m)\n",
    "        q_ = tf.concat([q * tf.sin(weight_index[:, :tgt_len, :] / m), q * tf.cos(weight_index[:, :tgt_len, :] / m)], axis=2)\n",
    "        k_ = tf.concat([k * tf.sin(weight_index[:, :src_len, :] / m), k * tf.cos(weight_index[:, :src_len, :] / m)], axis=2)\n",
    "\n",
    "        if self.causal:\n",
    "            kv_ = tf.einsum(\"nld,nlm->nldm\", k_, v)\n",
    "            kv_cum = tf.cumsum(kv_, axis=1)\n",
    "            qkv = tf.einsum(\"nld,nldm->nlm\", q_, kv_cum)\n",
    "            k_cum = tf.cumsum(k_, axis=1)\n",
    "            denom = tf.maximum(tf.einsum(\"nlm,nlm->nl\", q_, k_cum), eps)\n",
    "            attn_output = qkv / tf.expand_dims(denom, axis=2)\n",
    "        else:\n",
    "            kv_ = tf.einsum(\"nld,nlm->ndm\", k_, v)\n",
    "            z_ = 1 / tf.maximum(tf.einsum(\"nld,nd->nl\", q_, tf.reduce_sum(k_, axis=1)), eps)\n",
    "            attn_output = tf.einsum(\"nld,ndm,nl->nlm\", q_, kv_, z_)\n",
    "        \n",
    "        attn_output = tf.reshape(attn_output, (batch_size, self.num_heads, -1, self.head_dim))\n",
    "        attn_output = tf.reshape(tf.transpose(attn_output, (0, 2, 1, 3)), (batch_size, tgt_len, self.embed_dim))\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "dd433a6e-fff6-4761-8f99-84f330febd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(x, batch_size, num_heads, depth):\n",
    "    x = tf.reshape(x, (batch_size, -1, num_heads, depth))\n",
    "    return tf.reshape(tf.transpose(x, (0, 2, 1, 3)), (batch_size*num_heads, -1, depth))\n",
    "\n",
    "def get_index(seq_len):\n",
    "    return np.pi / 2 * tf.reshape(tf.range(1, seq_len + 1, dtype=tf.float32), (1, -1, 1))\n",
    "\n",
    "def call(query, num_heads, key=None, value=None, causal=False, eps=1e-6):\n",
    "    if key is None:\n",
    "        key = query\n",
    "    if value is None:\n",
    "        value = query\n",
    "\n",
    "    batch_size, tgt_len, embed_dim = tf.shape(query)\n",
    "    src_len = tf.shape(key)[1]\n",
    "    head_dim = embed_dim // num_heads\n",
    "\n",
    "    q = query\n",
    "    k = key\n",
    "    v = value\n",
    "    \n",
    "    # q = self.q_proj(query)\n",
    "    # k = self.q_proj(key)\n",
    "    # v = self.q_proj(value)\n",
    "\n",
    "    # multi-head reshape\n",
    "    q = split_heads(q, batch_size, num_heads, head_dim)\n",
    "    k = split_heads(k, batch_size, num_heads, head_dim)\n",
    "    v = split_heads(v, batch_size, num_heads, head_dim)\n",
    "\n",
    "    # cos transform\n",
    "    m = tf.cast(tf.maximum(src_len, tgt_len), dtype=tf.float32)\n",
    "    weight_index = get_index(m)\n",
    "    q_ = tf.concat([q * tf.sin(weight_index[:, :tgt_len, :] / m), q * tf.cos(weight_index[:, :tgt_len, :] / m)], axis=2)\n",
    "    k_ = tf.concat([k * tf.sin(weight_index[:, :src_len, :] / m), k * tf.cos(weight_index[:, :src_len, :] / m)], axis=2)\n",
    "    \n",
    "    if causal:\n",
    "        kv_ = tf.einsum(\"nld,nlm->nldm\", k_, v)\n",
    "        kv_cum = tf.cumsum(kv_, axis=1)\n",
    "        qkv = tf.einsum(\"nld,nldm->nlm\", q_, kv_cum)\n",
    "        k_cum = tf.cumsum(k_, axis=1)\n",
    "        denom = tf.maximum(tf.einsum(\"nlm,nlm->nl\", q_, k_cum), eps)\n",
    "        attn_output = qkv / tf.expand_dims(denom, axis=2)\n",
    "    else:\n",
    "        kv_ = tf.einsum(\"nld,nlm->ndm\", k_, v)\n",
    "        z_ = 1 / tf.maximum(tf.einsum(\"nld,nd->nl\", q_, tf.reduce_sum(k_, axis=1)), eps)\n",
    "        attn_output = tf.einsum(\"nld,ndm,nl->nlm\", q_, kv_, z_)\n",
    "        print(attn_output.shape)\n",
    "\n",
    "    attn_output = tf.reshape(tf.transpose(tf.reshape(attn_output, (batch_size, num_heads, -1, head_dim)), (0, 2, 1, 3)), (batch_size, tgt_len, -1))\n",
    "    return attn_output\n",
    "\n",
    "def torch_get_index(seq_len):\n",
    "    return np.pi / 2 * torch.arange(1, seq_len + 1).reshape(1, -1, 1)\n",
    "\n",
    "def forward(\n",
    "        query,\n",
    "        num_heads,\n",
    "        key=None,\n",
    "        value=None,\n",
    "        causal=False,\n",
    "        eps = 1e-6,\n",
    "    ):\n",
    "    \"\"\"Input shape: Sequence x Batch x Embedding\n",
    "    Args:\n",
    "        query (Tensor): `(L, N, E)` where L is the target sequence length, N is the batch size,\n",
    "        E is the embedding dimension.\n",
    "        key (Tensor): `(S, N, E)` where S is the source sequence length, N is the batch size,\n",
    "        E is the embedding dimension.\n",
    "        value (Tensor): `(S, N, E)` where S is the source sequence length, N is the batch size,\n",
    "        E is the embedding dimension.\n",
    "        attn_mask (Optional[Tensor], optional): typically used to implement causal attention, \n",
    "        where the mask prevents the attention from looking forward in time (default: None).\n",
    "    \"\"\"\n",
    "    if key == None:\n",
    "        key = query\n",
    "    if value == None:\n",
    "        value = query\n",
    "\n",
    "    num_heads = num_heads\n",
    "    tgt_len, bsz, embed_dim = query.size()\n",
    "    src_len = key.size(0)\n",
    "    head_dim = embed_dim // num_heads\n",
    "    \n",
    "    q = query\n",
    "    k = key\n",
    "    v = value\n",
    "\n",
    "    # multihead reshape\n",
    "    # (N * h, L, d)\n",
    "    q = q.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    # (N * h, S, d)\n",
    "    k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    # (N * h, S, d)\n",
    "    v = v.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    \n",
    "    # cos transform\n",
    "    m = max(src_len, tgt_len)\n",
    "    # get index and send to cuda\n",
    "    weight_index = torch_get_index(m).to(q)\n",
    "    # (N * h, L, 2 * d)\n",
    "    q_ = torch.cat([q * torch.sin(weight_index[:, :tgt_len, :] / m), q * torch.cos(weight_index[:, :tgt_len, :] / m)], dim=-1)\n",
    "    # (N * h, S, 2 * d)\n",
    "    k_ = torch.cat([k * torch.sin(weight_index[:, :src_len, :] / m), k * torch.cos(weight_index[:, :src_len, :] / m)], dim=-1)\n",
    "\n",
    "    if causal:\n",
    "        ## Need to improve speed!\n",
    "        # (N * h, L, 2 * d) (N * h, L, d) -> (N * h, L, h, 2 * d, d)\n",
    "        kv_ = torch.einsum(\"nld,nlm->nldm\", k_, v)\n",
    "        # (N * h, L, 2 * d, d) -> (N * h, L, 2 * d, d)\n",
    "        kv_cum = torch.cumsum(kv_, dim=1)\n",
    "        # (N * h, L, 2 * d) (N * h, L, 2 * d, d) -> (N * h, L, d)\n",
    "        qkv = torch.einsum(\"nld,nldm->nlm\", q_, kv_cum)\n",
    "        # (N * h, L, 2 * d) -> (N * h, L, 2 * d)\n",
    "        k_cum = torch.cumsum(k_, dim=1)\n",
    "        # (N * h, L, 2 * d) (N * h, L, 2 * d) -> (N * h, L)\n",
    "        denom = torch.clamp_min(torch.einsum(\"nlm,nlm->nl\", q_, k_cum), eps)\n",
    "        # (N * h, L, d) (N * h, L, 1) -> (N * h, L, d)\n",
    "        attn_output = qkv / denom.unsqueeze(-1)\n",
    "        # (N * h, L, d) -> (L, N * h, d) -> (L, N, E)\n",
    "        attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, -1)\n",
    "    else:\n",
    "        # (N * h, L, 2 * d) (N * h, L, d) -> (N * h, 2 * d, d)\n",
    "        kv_ = torch.einsum('nld,nlm->ndm', k_, v)\n",
    "        # (N * h, L, 2 * d) (N * h, 2 * d) -> (N * h, L)\n",
    "        z_ = 1 / torch.clamp_min(torch.einsum('nld,nd->nl', q_, torch.sum(k_, axis=1)), eps)\n",
    "        # (N * h, L, 2 * d) (N * h, d, 2 * d) (N * h, L) -> (N * h, L, d)\n",
    "        attn_output = torch.einsum('nld,ndm,nl->nlm', q_, kv_, z_)\n",
    "        # (N * h, L, d) -> (L, N * h, d) -> (L, N, E)\n",
    "        print(attn_output.shape)\n",
    "        attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, -1)\n",
    "\n",
    "    return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "8e28136a-2589-41d7-bc23-b644d67110f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "num_heads = 4\n",
    "embed_dim = 128\n",
    "length = 148\n",
    "head_dim = embed_dim // num_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "5284af40-1c6d-484f-bae3-301aef6ed022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 148, 128)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(embed_dim*batch_size*length, dtype=np.float32).reshape(batch_size, length, embed_dim)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "3df3b89c-1dae-4304-af6a-45b833384436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([512, 148, 128]), torch.Size([148, 512, 128]))"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atf = tf.constant(a)\n",
    "at = torch.Tensor(a).transpose(0, 1)\n",
    "atf.shape, at.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "0f5634d5-5d12-41cc-8615-3e50a9189c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 148, 32)\n",
      "torch.Size([2048, 148, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30.0"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(call(atf, num_heads) - forward(at, num_heads).transpose(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d7d2763f-dfd1-4bab-9056-39de8a67fc51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 20), dtype=float32, numpy=\n",
       "array([[[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00, -1.9073486e-06, -1.9073486e-06,  0.0000000e+00,\n",
       "          1.9073486e-06,  3.8146973e-06,  3.8146973e-06,  5.7220459e-06,\n",
       "          5.7220459e-06,  1.9073486e-06,  0.0000000e+00,  1.9073486e-06,\n",
       "         -3.8146973e-06,  0.0000000e+00,  0.0000000e+00,  1.9073486e-06,\n",
       "          1.9073486e-06,  3.8146973e-06,  3.8146973e-06,  3.8146973e-06],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  3.8146973e-06,\n",
       "          0.0000000e+00,  0.0000000e+00, -7.6293945e-06,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  3.8146973e-06,  3.8146973e-06,\n",
       "          3.8146973e-06,  3.8146973e-06,  0.0000000e+00,  0.0000000e+00,\n",
       "         -3.8146973e-06,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00, -7.6293945e-06,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  7.6293945e-06,  7.6293945e-06,\n",
       "          7.6293945e-06,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00]],\n",
       "\n",
       "       [[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  1.5258789e-05,  7.6293945e-06,  1.5258789e-05,\n",
       "          0.0000000e+00, -7.6293945e-06,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  1.5258789e-05,  0.0000000e+00,\n",
       "          7.6293945e-06,  0.0000000e+00, -7.6293945e-06, -1.5258789e-05,\n",
       "          0.0000000e+00,  0.0000000e+00, -1.5258789e-05,  0.0000000e+00],\n",
       "        [ 7.6293945e-06,  1.5258789e-05,  1.5258789e-05,  2.2888184e-05,\n",
       "          7.6293945e-06,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         -1.5258789e-05,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  1.5258789e-05,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  3.0517578e-05,  4.5776367e-05,  1.5258789e-05,\n",
       "          3.0517578e-05,  3.0517578e-05,  0.0000000e+00, -1.5258789e-05,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  1.5258789e-05,  1.5258789e-05,\n",
       "          1.5258789e-05,  1.5258789e-05,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00, -1.5258789e-05,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call(atf, num_heads, causal=True) - forward(at, num_heads, causal=True).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66ebda52-a6e5-493c-b96d-e07d639ebd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "atf = tf.transpose(tf.reshape(a, (-1, batch_size*num_heads, head_dim)), (1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2f41846-abe8-44b1-b201-e2c657a191c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_index = np.pi / 2 * tf.reshape(tf.range(1, length + 1, dtype=tf.float32), (1, -1, 1))\n",
    "q_ = tf.concat([atf * tf.sin(weight_index[:, :length, :] / length), atf * tf.cos(weight_index[:, :length, :] / length)], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63dbd3aa-100d-45f7-a9cd-4110cd54129d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8, 5), dtype=float32, numpy=\n",
       "array([[  2337.1118,  66363.09  , 152012.55  , 237462.38  , 299471.97  ],\n",
       "       [  8728.424 ,  79309.57  , 171110.16  , 260841.7   , 323981.22  ],\n",
       "       [ 16033.954 ,  93330.766 , 191337.83  , 285295.8   , 349404.72  ],\n",
       "       [ 24253.697 , 108426.695 , 212695.5   , 310824.6   , 375742.5   ],\n",
       "       [ 33387.67  , 124597.36  , 235183.25  , 337428.12  , 402994.44  ],\n",
       "       [ 43435.855 , 141842.77  , 258801.03  , 365106.44  , 431160.62  ],\n",
       "       [ 54398.266 , 160162.88  , 283548.8   , 393859.44  , 460240.97  ],\n",
       "       [ 66274.89  , 179557.72  , 309426.66  , 423687.2   , 490235.6   ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.maximum(tf.einsum(\"nld,nd->nl\", q_, tf.reduce_sum(q_, axis=1)), 1e-6)\n",
    "# tf.maximum(tf.einsum(\"nld,nd->nl\", q_, tf.reduce_sum(q_, axis=1)), 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94ba9456-234d-408e-95a6-5316e150ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "at = torch.Tensor(a)\n",
    "at = at.contiguous().view(-1, batch_size*num_heads, head_dim).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "523ddb94-2ff6-4767-afa9-fd694427d5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_index = np.pi / 2 * torch.arange(1, length + 1).reshape(1, -1, 1)\n",
    "q_ = torch.cat([at * torch.sin(weight_index[:, :length, :] / length), at * torch.cos(weight_index[:, :length, :] / length)], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9676d34d-8fca-4b0d-909e-c10ee3d499de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  2337.1118,  66363.0859, 152012.5469, 237462.3750, 299471.9375],\n",
       "        [  8728.4238,  79309.5781, 171110.1562, 260841.7031, 323981.2188],\n",
       "        [ 16033.9531,  93330.7656, 191337.8125, 285295.8125, 349404.6875],\n",
       "        [ 24253.6992, 108426.7109, 212695.5156, 310824.5938, 375742.5000],\n",
       "        [ 33387.6719, 124597.3672, 235183.2656, 337428.1250, 402994.4375],\n",
       "        [ 43435.8555, 141842.7656, 258801.0312, 365106.4375, 431160.6250],\n",
       "        [ 54398.2695, 160162.8906, 283548.8125, 393859.4375, 460240.9688],\n",
       "        [ 66274.8906, 179557.7188, 309426.6562, 423687.1875, 490235.5938]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.clamp_min(torch.einsum(\"nld,nd->nl\", q_, torch.sum(q_, axis=1)), 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201a6114-166f-4a9e-8b8a-80ada7801589",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
