{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b9d66e2-36cb-47ad-a405-f8130d3ff2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0217f1e5-d006-4349-b510-aa1aaf2f0324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import tf_utilities as tfu\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Iterable, Generator, Optional\n",
    "import time\n",
    "import wandb\n",
    "\n",
    "from dnadb.datasets import Greengenes, Silva\n",
    "from dnadb import dna, fasta, sample, taxonomy\n",
    "\n",
    "from deepdna.data.dataset import Dataset\n",
    "from deepdna.data.tokenizers import AbstractTaxonomyTokenizer, TopDownTaxonomyTokenizer\n",
    "from deepdna.nn.models import custom_model, dnabert, load_model, taxonomy as tax_models\n",
    "from deepdna.nn.utils import encapsulate_model\n",
    "from deepdna.nn import layers, functional, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "adbdd12e-425e-4f05-86a1-3f2349186ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')],\n",
       " [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfu.devices.select_gpu(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a507324-98a6-4e64-9a7e-41e8b23341b0",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05f341b7-adf1-4ff3-8002-1e20a2ff5a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(\"/home/dwl2x/work/Datasets/Silva2/0\")\n",
    "train_fastas = tuple(map(sample.load_fasta, dataset.fasta_dbs(Dataset.Split.Train)))\n",
    "train_tax = tuple(map(taxonomy.TaxonomyDb, dataset.taxonomy_dbs(Dataset.Split.Train)))\n",
    "test_fastas = tuple(map(sample.load_fasta, dataset.fasta_dbs(Dataset.Split.Test)))\n",
    "test_tax = tuple(map(taxonomy.TaxonomyDb, dataset.taxonomy_dbs(Dataset.Split.Test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "caee8e83-27e4-48d3-8970-fd14bc186748",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in test_tax[0]:\n",
    "    assert train_tax[0].contains_label(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95906939-7cf1-452a-9d70-41c7276c83d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TopDownTaxonomyTokenizer(depth=6)\n",
    "for db in train_tax:\n",
    "    tokenizer.add_labels(db)\n",
    "tokenizer.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e6d1698-6c9a-450a-aaab-6610f58be13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepdna.data.samplers import SampleSampler, SequenceSampler\n",
    "from deepdna.nn.data_generators import _encode_sequences, BatchGenerator\n",
    "from typing import Any, cast\n",
    "\n",
    "class SequenceTaxonomyGenerator(BatchGenerator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        fasta_taxonomy_pairs: Iterable[tuple[sample.FastaSample, taxonomy.TaxonomyDb]],\n",
    "        sequence_length: int,\n",
    "        taxonomy_tokenizer: AbstractTaxonomyTokenizer,\n",
    "        kmer: int = 1,\n",
    "        subsample_size: int|None = None,\n",
    "        batch_size: int = 32,\n",
    "        batches_per_epoch: int = 100,\n",
    "        augment_slide: bool = True,\n",
    "        augment_ambiguous_bases: bool = True,\n",
    "        balance: bool = False,\n",
    "        shuffle: bool = True,\n",
    "        rng: np.random.Generator = np.random.default_rng()\n",
    "    ):\n",
    "        super().__init__(\n",
    "            batch_size=batch_size,\n",
    "            batches_per_epoch=batches_per_epoch,\n",
    "            shuffle=shuffle,\n",
    "            rng=rng\n",
    "        )\n",
    "        fasta_samples, taxonomy_dbs = zip(*fasta_taxonomy_pairs)\n",
    "        self.sample_sampler = SampleSampler(cast(tuple[sample.FastaSample, ...], fasta_samples))\n",
    "        self.sequence_sampler = SequenceSampler(sequence_length, augment_slide)\n",
    "        self.taxonomy_dbs: tuple[taxonomy.TaxonomyDb, ...] = cast(Any, taxonomy_dbs)\n",
    "        self.kmer = kmer\n",
    "        self.taxonomy_tokenizer = taxonomy_tokenizer\n",
    "        self.subsample_size = subsample_size\n",
    "        self.augment_ambiguous_bases = augment_ambiguous_bases\n",
    "        self.balance = balance\n",
    "\n",
    "    @property\n",
    "    def sequence_length(self) -> int:\n",
    "        return self.sequence_sampler.sequence_length\n",
    "\n",
    "    def generate_batch(\n",
    "        self,\n",
    "        rng: np.random.Generator\n",
    "    ) -> tuple[npt.NDArray[np.int32], npt.NDArray[np.int32]]:\n",
    "        subsample_size = self.subsample_size or 1\n",
    "        sequences = np.empty((self.batch_size, subsample_size), dtype=f\"<U{self.sequence_length}\")\n",
    "        sample_ids = np.empty(self.batch_size, dtype=np.int32)\n",
    "        sequence_ids = [None] * self.batch_size\n",
    "        label_ids = np.empty((self.batch_size, subsample_size, self.taxonomy_tokenizer.depth), dtype=np.int32)\n",
    "        samples = self.sample_sampler.sample_with_ids(self.batch_size, self.balance, rng)\n",
    "        for i, (sample_id, sample) in enumerate(samples):\n",
    "            tax_db = self.taxonomy_dbs[sample_id]\n",
    "            sequence_info = tuple(self.sequence_sampler.sample_with_ids(sample, subsample_size, rng))\n",
    "            sequence_ids[i], sequences[i] = zip(*sequence_info)\n",
    "            sample_ids[i] = sample_id\n",
    "            label_ids[i] = [self.taxonomy_tokenizer.tokenize_label(tax_db.fasta_id_to_label(fasta_id)) for fasta_id in sequence_ids[i]]\n",
    "        sequences = _encode_sequences(sequences, self.augment_ambiguous_bases, self.rng)\n",
    "        if self.subsample_size is None:\n",
    "            sequences = np.squeeze(sequences, axis=1)\n",
    "            label_ids = np.squeeze(label_ids, axis=1)\n",
    "        sequences = sequences.astype(np.int32)\n",
    "        if self.kmer > 1:\n",
    "            sequences = dna.encode_kmers(sequences, self.kmer, not self.augment_ambiguous_bases).astype(np.int32) # type: ignore\n",
    "        return sample_ids, sequence_ids, sequences, tuple(label_ids.T)[-1]\n",
    "\n",
    "    def reduce_batch(self, batch):\n",
    "        # remove sample IDs and sequence IDs\n",
    "        return batch[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0be0fe5-be19-4329-a04f-baf13a24eed2",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc66d20f-4d77-459a-a838-3367d852ea2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msirdavidludwig\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dwl2x/work/deep-dna/notebooks/wandb/run-20230908_234313-9hk5aorr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sirdavidludwig/dnabert-taxonomy/runs/9hk5aorr' target=\"_blank\">topdown 64d-150l</a></strong> to <a href='https://wandb.ai/sirdavidludwig/dnabert-taxonomy' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sirdavidludwig/dnabert-taxonomy' target=\"_blank\">https://wandb.ai/sirdavidludwig/dnabert-taxonomy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sirdavidludwig/dnabert-taxonomy/runs/9hk5aorr' target=\"_blank\">https://wandb.ai/sirdavidludwig/dnabert-taxonomy/runs/9hk5aorr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# api = wandb.Api()\n",
    "run = wandb.init(project=\"dnabert-taxonomy\", name=\"topdown 64d-150l\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7cba1f3-9b72-4d69-8771-2e892441fe23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   4 of 4 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "# path = api.artifact(\"sirdavidludwig/dnabert-pretrain/dnabert-pretrain-silva-64:v3\").download()\n",
    "path = run.use_artifact(\"sirdavidludwig/dnabert-pretrain/dnabert-pretrain-silva-64:v3\").download()\n",
    "dnabert_model = load_model(path, dnabert.DnaBertPretrainModel).base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "baca7f3e-ca77-4814-b8e7-6b5b8ac41173",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = dnabert.DnaBertEncoderModel(dnabert_model, 256)\n",
    "model = tax_models.TopDownTaxonomyClassificationModel(encoder, tokenizer)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c6be2284-4824-46c3-b91e-84978433c3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_args = dict(\n",
    "    sequence_length = 150,\n",
    "    kmer = 3,\n",
    "    taxonomy_tokenizer = model.taxonomy_tokenizer,\n",
    "    subsample_size=None,\n",
    "    batch_size = 256,\n",
    ")\n",
    "\n",
    "train_data = SequenceTaxonomyGenerator(\n",
    "    zip(train_fastas, train_tax),\n",
    "    batches_per_epoch=100,\n",
    "    **common_args)\n",
    "test_data = SequenceTaxonomyGenerator(\n",
    "    zip(test_fastas, test_tax),\n",
    "    batches_per_epoch=20,\n",
    "    **common_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "605fdf84-d31c-4821-9622-3afef0c86de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_callback = wandb.keras.WandbCallback(save_model=False)\n",
    "wandb_callback.save_model_as_artifact = False\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"logs/models/dnabert_taxonomy_topdown\", save_best=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73899004-8c5d-429b-b7ed-7b6c66606dad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3501/4000\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.9723 - sparse_categorical_accuracy: 0.7764"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_2_layer_call_fn, embedding_2_layer_call_and_return_conditional_losses, layer_normalization_32_layer_call_fn, layer_normalization_32_layer_call_and_return_conditional_losses, layer_normalization_33_layer_call_fn while saving (showing 5 of 178). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: logs/models/dnabert_taxonomy_topdown/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: logs/models/dnabert_taxonomy_topdown/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 46s 459ms/step - loss: 0.9723 - sparse_categorical_accuracy: 0.7764 - val_loss: 0.7770 - val_sparse_categorical_accuracy: 0.8207\n",
      "Epoch 3502/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/engine/functional.py:1384: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "/opt/conda/lib/python3.10/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80/100 [=======================>......] - ETA: 6s - loss: 0.9537 - sparse_categorical_accuracy: 0.7813"
     ]
    }
   ],
   "source": [
    "model.fit(train_data, validation_data=test_data, epochs=4000, initial_epoch=3500, callbacks=[wandb_callback, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc83aca8-a7c1-4ff4-bd0c-ea17f59ae8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"logs/models/dnabert_taxonomy_topdown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4500eed-a749-4459-9ad0-b3e1dbc94f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = wandb.Artifact(name=\"dnabert-taxonomy-topdown-64d-150l\", type=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "567113a8-5853-4a4f-942d-7985a922c2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./logs/models/dnabert_taxonomy_naive)... Done. 0.1s\n"
     ]
    }
   ],
   "source": [
    "a.add_dir(\"logs/models/dnabert_taxonomy_topdown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2054e2e-f842-4fd5-9e76-683c38cdb101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Artifact dnabert-taxonomy-naive-64d-150l>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.log_artifact(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcd5f589-bebc-4848-98ae-17576ef6ccff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▆▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparse_categorical_accuracy</td><td>▁▃▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>val_loss</td><td>█▆▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_sparse_categorical_accuracy</td><td>▁▃▄▄▅▅▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>2693</td></tr><tr><td>best_val_loss</td><td>0.84462</td></tr><tr><td>epoch</td><td>2999</td></tr><tr><td>loss</td><td>1.0782</td></tr><tr><td>sparse_categorical_accuracy</td><td>0.75941</td></tr><tr><td>val_loss</td><td>0.89255</td></tr><tr><td>val_sparse_categorical_accuracy</td><td>0.79707</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">64d-150l</strong> at: <a href='https://wandb.ai/sirdavidludwig/dnabert-taxonomy-naive/runs/8mchqa82' target=\"_blank\">https://wandb.ai/sirdavidludwig/dnabert-taxonomy-naive/runs/8mchqa82</a><br/>Synced 6 W&B file(s), 1 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230817_230617-8mchqa82/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b4366a-bde8-46fc-9cf8-8a3c74d8fd7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
