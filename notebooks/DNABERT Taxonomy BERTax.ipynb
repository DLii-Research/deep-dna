{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b9d66e2-36cb-47ad-a405-f8130d3ff2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0217f1e5-d006-4349-b510-aa1aaf2f0324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import tf_utilities as tfu\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Iterable, Generator, Optional\n",
    "import time\n",
    "import wandb\n",
    "\n",
    "from dnadb.datasets import Greengenes, Silva\n",
    "from dnadb import dna, fasta, sample, taxonomy\n",
    "\n",
    "from deepdna.data.dataset import Dataset\n",
    "from deepdna.data.tokenizers import AbstractTaxonomyTokenizer, NaiveTaxonomyTokenizer\n",
    "from deepdna.nn.models import custom_model, dnabert, load_model, taxonomy as tax_models\n",
    "from deepdna.nn.utils import encapsulate_model\n",
    "from deepdna.nn import layers, functional, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adbdd12e-425e-4f05-86a1-3f2349186ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')],\n",
       " [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfu.devices.select_gpu(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a507324-98a6-4e64-9a7e-41e8b23341b0",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05f341b7-adf1-4ff3-8002-1e20a2ff5a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(\"/home/dwl2x/work/Datasets/Silva2/0\")\n",
    "train_fastas = tuple(map(sample.load_fasta, dataset.fasta_dbs(Dataset.Split.Train)))\n",
    "train_tax = tuple(map(taxonomy.TaxonomyDb, dataset.taxonomy_dbs(Dataset.Split.Train)))\n",
    "test_fastas = tuple(map(sample.load_fasta, dataset.fasta_dbs(Dataset.Split.Test)))\n",
    "test_tax = tuple(map(taxonomy.TaxonomyDb, dataset.taxonomy_dbs(Dataset.Split.Test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caee8e83-27e4-48d3-8970-fd14bc186748",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in test_tax[0]:\n",
    "    assert train_tax[0].contains_label(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95906939-7cf1-452a-9d70-41c7276c83d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = NaiveTaxonomyTokenizer(depth=6)\n",
    "for db in train_tax:\n",
    "    tokenizer.add_labels(db)\n",
    "tokenizer.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e6d1698-6c9a-450a-aaab-6610f58be13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepdna.data.samplers import SampleSampler, SequenceSampler\n",
    "from deepdna.nn.data_generators import _encode_sequences, BatchGenerator\n",
    "from typing import Any, cast\n",
    "\n",
    "class SequenceTaxonomyGenerator(BatchGenerator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        fasta_taxonomy_pairs: Iterable[tuple[sample.FastaSample, taxonomy.TaxonomyDb]],\n",
    "        sequence_length: int,\n",
    "        taxonomy_tokenizer: AbstractTaxonomyTokenizer,\n",
    "        kmer: int = 1,\n",
    "        subsample_size: int|None = None,\n",
    "        batch_size: int = 32,\n",
    "        batches_per_epoch: int = 100,\n",
    "        augment_slide: bool = True,\n",
    "        augment_ambiguous_bases: bool = True,\n",
    "        balance: bool = False,\n",
    "        shuffle: bool = True,\n",
    "        rng: np.random.Generator = np.random.default_rng()\n",
    "    ):\n",
    "        super().__init__(\n",
    "            batch_size=batch_size,\n",
    "            batches_per_epoch=batches_per_epoch,\n",
    "            shuffle=shuffle,\n",
    "            rng=rng\n",
    "        )\n",
    "        fasta_samples, taxonomy_dbs = zip(*fasta_taxonomy_pairs)\n",
    "        self.sample_sampler = SampleSampler(cast(tuple[sample.FastaSample, ...], fasta_samples))\n",
    "        self.sequence_sampler = SequenceSampler(sequence_length, augment_slide)\n",
    "        self.taxonomy_dbs: tuple[taxonomy.TaxonomyDb, ...] = cast(Any, taxonomy_dbs)\n",
    "        self.kmer = kmer\n",
    "        self.taxonomy_tokenizer = taxonomy_tokenizer\n",
    "        self.subsample_size = subsample_size\n",
    "        self.augment_ambiguous_bases = augment_ambiguous_bases\n",
    "        self.balance = balance\n",
    "\n",
    "    @property\n",
    "    def sequence_length(self) -> int:\n",
    "        return self.sequence_sampler.sequence_length\n",
    "\n",
    "    def generate_batch(\n",
    "        self,\n",
    "        rng: np.random.Generator\n",
    "    ) -> tuple[npt.NDArray[np.int32], npt.NDArray[np.int32]]:\n",
    "        subsample_size = self.subsample_size or 1\n",
    "        sequences = np.empty((self.batch_size, subsample_size), dtype=f\"<U{self.sequence_length}\")\n",
    "        sample_ids = np.empty(self.batch_size, dtype=np.int32)\n",
    "        sequence_ids = [None] * self.batch_size\n",
    "        label_ids = np.empty((self.batch_size, subsample_size, self.taxonomy_tokenizer.depth), dtype=np.int32)\n",
    "        samples = self.sample_sampler.sample_with_ids(self.batch_size, self.balance, rng)\n",
    "        for i, (sample_id, sample) in enumerate(samples):\n",
    "            tax_db = self.taxonomy_dbs[sample_id]\n",
    "            sequence_info = tuple(self.sequence_sampler.sample_with_ids(sample, subsample_size, rng))\n",
    "            sequence_ids[i], sequences[i] = zip(*sequence_info)\n",
    "            sample_ids[i] = sample_id\n",
    "            label_ids[i] = [self.taxonomy_tokenizer.tokenize_label(tax_db.fasta_id_to_label(fasta_id)) for fasta_id in sequence_ids[i]]\n",
    "        sequences = _encode_sequences(sequences, self.augment_ambiguous_bases, self.rng)\n",
    "        if self.subsample_size is None:\n",
    "            sequences = np.squeeze(sequences, axis=1)\n",
    "            label_ids = np.squeeze(label_ids, axis=1)\n",
    "        sequences = sequences.astype(np.int32)\n",
    "        if self.kmer > 1:\n",
    "            sequences = dna.encode_kmers(sequences, self.kmer, not self.augment_ambiguous_bases).astype(np.int32) # type: ignore\n",
    "        return sample_ids, sequence_ids, sequences, tuple(label_ids.T)\n",
    "\n",
    "    def reduce_batch(self, batch):\n",
    "        # remove sample IDs and sequence IDs\n",
    "        return batch[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0be0fe5-be19-4329-a04f-baf13a24eed2",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc66d20f-4d77-459a-a838-3367d852ea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "# run = wandb.init(project=\"dnabert-taxonomy-topdown\", name=\"64d-150l\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7cba1f3-9b72-4d69-8771-2e892441fe23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   4 of 4 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "path = api.artifact(\"sirdavidludwig/dnabert-pretrain/dnabert-pretrain-silva-64:v3\").download()\n",
    "# path = run.use_artifact(\"sirdavidludwig/dnabert-pretrain/dnabert-pretrain-silva-64:v3\").download()\n",
    "dnabert_model = load_model(path, dnabert.DnaBertPretrainModel).base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baca7f3e-ca77-4814-b8e7-6b5b8ac41173",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = dnabert.DnaBertEncoderModel(dnabert_model, 256)\n",
    "model = tax_models.BertaxTaxonomyClassificationModel(encoder, tokenizer)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6be2284-4824-46c3-b91e-84978433c3cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m common_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m      2\u001b[0m     sequence_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m150\u001b[39m,\n\u001b[1;32m      3\u001b[0m     kmer \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m----> 4\u001b[0m     taxonomy_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mtaxonomy_tokenizer,\n\u001b[1;32m      5\u001b[0m     subsample_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m train_data \u001b[38;5;241m=\u001b[39m SequenceTaxonomyGenerator(\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mzip\u001b[39m(train_fastas, train_tax),\n\u001b[1;32m     11\u001b[0m     batches_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcommon_args)\n\u001b[1;32m     13\u001b[0m test_data \u001b[38;5;241m=\u001b[39m SequenceTaxonomyGenerator(\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mzip\u001b[39m(test_fastas, test_tax),\n\u001b[1;32m     15\u001b[0m     batches_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcommon_args)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "common_args = dict(\n",
    "    sequence_length = 150,\n",
    "    kmer = 3,\n",
    "    taxonomy_tokenizer = model.taxonomy_tokenizer,\n",
    "    subsample_size=None,\n",
    "    batch_size = 256,\n",
    ")\n",
    "\n",
    "train_data = SequenceTaxonomyGenerator(\n",
    "    zip(train_fastas, train_tax),\n",
    "    batches_per_epoch=100,\n",
    "    **common_args)\n",
    "test_data = SequenceTaxonomyGenerator(\n",
    "    zip(test_fastas, test_tax),\n",
    "    batches_per_epoch=20,\n",
    "    **common_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "605fdf84-d31c-4821-9622-3afef0c86de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb_callback = wandb.keras.WandbCallback(save_model=False)\n",
    "# wandb_callback.save_model_as_artifact = False\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"logs/models/dnabert_taxonomy_bertax\", save_best=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73899004-8c5d-429b-b7ed-7b6c66606dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - loss: 25.2832 - kingdom_loss: 0.3560 - phylum_loss: 3.9490 - class_loss: 4.3717 - order_loss: 4.9466 - family_loss: 5.3141 - genus_loss: 6.3458 - kingdom_sparse_categorical_accuracy: 0.8901 - phylum_sparse_categorical_accuracy: 0.2277 - class_sparse_categorical_accuracy: 0.2091 - order_sparse_categorical_accuracy: 0.1225 - family_sparse_categorical_accuracy: 0.1114 - genus_sparse_categorical_accuracy: 0.1118"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, layer_normalization_layer_call_fn, layer_normalization_layer_call_and_return_conditional_losses, layer_normalization_1_layer_call_fn while saving (showing 5 of 178). These functions will not be directly callable after loading.\n",
      "2023-09-08 20:21:54.261820: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 374475792 exceeds 10% of free system memory.\n",
      "2023-09-08 20:21:54.410859: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 374475792 exceeds 10% of free system memory.\n",
      "2023-09-08 20:21:54.551404: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 374475792 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: logs/models/dnabert_taxonomy_bertax/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: logs/models/dnabert_taxonomy_bertax/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 48s 435ms/step - loss: 25.2832 - kingdom_loss: 0.3560 - phylum_loss: 3.9490 - class_loss: 4.3717 - order_loss: 4.9466 - family_loss: 5.3141 - genus_loss: 6.3458 - kingdom_sparse_categorical_accuracy: 0.8901 - phylum_sparse_categorical_accuracy: 0.2277 - class_sparse_categorical_accuracy: 0.2091 - order_sparse_categorical_accuracy: 0.1225 - family_sparse_categorical_accuracy: 0.1114 - genus_sparse_categorical_accuracy: 0.1118 - val_loss: 18.8027 - val_kingdom_loss: 0.1186 - val_phylum_loss: 2.4250 - val_class_loss: 2.6426 - val_order_loss: 3.6533 - val_family_loss: 4.3861 - val_genus_loss: 5.5771 - val_kingdom_sparse_categorical_accuracy: 0.9631 - val_phylum_sparse_categorical_accuracy: 0.4309 - val_class_sparse_categorical_accuracy: 0.4256 - val_order_sparse_categorical_accuracy: 0.2445 - val_family_sparse_categorical_accuracy: 0.1896 - val_genus_sparse_categorical_accuracy: 0.1838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/engine/functional.py:1384: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "/opt/conda/lib/python3.10/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f09ec395b10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, validation_data=test_data, epochs=1, initial_epoch=0, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc83aca8-a7c1-4ff4-bd0c-ea17f59ae8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, layer_normalization_layer_call_fn, layer_normalization_layer_call_and_return_conditional_losses, layer_normalization_1_layer_call_fn while saving (showing 5 of 178). These functions will not be directly callable after loading.\n",
      "2023-09-08 20:22:12.860141: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 374475792 exceeds 10% of free system memory.\n",
      "2023-09-08 20:22:13.009454: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 374475792 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: logs/models/dnabert_taxonomy_bertax/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: logs/models/dnabert_taxonomy_bertax/assets\n",
      "/opt/conda/lib/python3.10/site-packages/keras/engine/functional.py:1384: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "/opt/conda/lib/python3.10/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n"
     ]
    }
   ],
   "source": [
    "model.save(\"logs/models/dnabert_taxonomy_bertax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ec0f9a5-4012-4588-905d-16f52aa07c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-08 20:23:26.786024: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 374475792 exceeds 10% of free system memory.\n",
      "2023-09-08 20:23:26.786059: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 374475792 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"logs/models/dnabert_taxonomy_bertax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "e41df8db-aa07-4c06-82cf-9104866ed7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [np.taxonomy_tokenizermax(row, axis=1) for row in model.predict(test_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bce5b28-5d6e-4b40-93fc-09458219f874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, layer_normalization_layer_call_fn, layer_normalization_layer_call_and_return_conditional_losses, layer_normalization_1_layer_call_fn while saving (showing 5 of 178). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: logs/models/dnabert_taxonomy_naive_bak/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: logs/models/dnabert_taxonomy_naive_bak/assets\n",
      "/opt/conda/lib/python3.10/site-packages/keras/engine/functional.py:1384: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "/opt/conda/lib/python3.10/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n"
     ]
    }
   ],
   "source": [
    "model.save(\"logs/models/dnabert_taxonomy_bertax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4500eed-a749-4459-9ad0-b3e1dbc94f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = wandb.Artifact(name=\"dnabert-taxonomy-bertax-64d-150l\", type=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "567113a8-5853-4a4f-942d-7985a922c2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./logs/models/dnabert_taxonomy_naive)... Done. 0.1s\n"
     ]
    }
   ],
   "source": [
    "a.add_dir(\"logs/models/dnabert_taxonomy_bertax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2054e2e-f842-4fd5-9e76-683c38cdb101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Artifact dnabert-taxonomy-naive-64d-150l>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.log_artifact(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcd5f589-bebc-4848-98ae-17576ef6ccff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▆▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparse_categorical_accuracy</td><td>▁▃▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>val_loss</td><td>█▆▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_sparse_categorical_accuracy</td><td>▁▃▄▄▅▅▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>2693</td></tr><tr><td>best_val_loss</td><td>0.84462</td></tr><tr><td>epoch</td><td>2999</td></tr><tr><td>loss</td><td>1.0782</td></tr><tr><td>sparse_categorical_accuracy</td><td>0.75941</td></tr><tr><td>val_loss</td><td>0.89255</td></tr><tr><td>val_sparse_categorical_accuracy</td><td>0.79707</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">64d-150l</strong> at: <a href='https://wandb.ai/sirdavidludwig/dnabert-taxonomy-naive/runs/8mchqa82' target=\"_blank\">https://wandb.ai/sirdavidludwig/dnabert-taxonomy-naive/runs/8mchqa82</a><br/>Synced 6 W&B file(s), 1 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230817_230617-8mchqa82/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b4366a-bde8-46fc-9cf8-8a3c74d8fd7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
